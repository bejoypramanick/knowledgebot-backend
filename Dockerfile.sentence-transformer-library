# Use prebuilt wheels approach - no Rust compilation needed
FROM public.ecr.aws/lambda/python:3.11

# Set cache directories to PERSISTENT locations in the image filesystem
ENV TRANSFORMERS_CACHE=/opt/models/transformers_cache
ENV HF_HOME=/opt/models/huggingface
ENV SENTENCE_TRANSFORMERS_HOME=/opt/models/sentence_transformers
ENV SENTENCE_TRANSFORMERS_CACHE=/opt/models/sentence_transformers_cache

# Copy requirements and install using prebuilt wheels
COPY requirements-sentence-transformer-library.txt ${LAMBDA_TASK_ROOT}/

# Install dependencies with prebuilt wheels (no compilation needed)
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir \
    --extra-index-url https://download.pytorch.org/whl/cpu \
    --find-links https://download.pytorch.org/whl/torch_stable.html \
    -r requirements-sentence-transformer-library.txt

# Create PERSISTENT cache directories in the image filesystem
RUN mkdir -p /opt/models/transformers_cache /opt/models/huggingface /opt/models/sentence_transformers /opt/models/sentence_transformers_cache

# Pre-download and cache the model during build time to PERSISTENT location
RUN python -c "import os; os.environ['TRANSFORMERS_CACHE']='/opt/models/transformers_cache'; os.environ['HF_HOME']='/opt/models/huggingface'; os.environ['SENTENCE_TRANSFORMERS_CACHE']='/opt/models/sentence_transformers_cache'; from sentence_transformers import SentenceTransformer; model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder='/opt/models/sentence_transformers_cache'); print('Model downloaded and cached successfully'); print('Model info:', model)"

# Verify the model is properly cached in PERSISTENT location
RUN ls -la /opt/models/sentence_transformers_cache/ && echo "Cache contents:" && find /opt/models/sentence_transformers_cache -type f | head -10

# Copy the library handler
COPY microservices/sentence-transformer-library-handler.py ${LAMBDA_TASK_ROOT}/

# Set the CMD to your handler
CMD ["sentence-transformer-library-handler.lambda_handler"]
