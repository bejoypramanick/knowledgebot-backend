name: Build and Deploy Unified Microservices

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      force_rebuild_all:
        description: 'Force rebuild all services (ignore change detection)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.ap-south-1.amazonaws.com
  ECR_REPOSITORY_PREFIX: knowledgebot

jobs:
  # Cleanup before build
  cleanup:
    runs-on: ubuntu-latest
    environment: chatbot
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Verify AWS credentials
      run: |
        echo "üîç Verifying AWS credentials..."
        aws sts get-caller-identity
        echo "‚úÖ AWS credentials verified successfully"

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Clean up GitHub Actions cache and artifacts
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        echo "üßπ Cleaning up GitHub Actions cache and artifacts..."
        
        # Clean up GitHub Actions cache (older than 7 days)
        gh cache list --limit 1000 | while read line; do
          cache_id=$(echo "$line" | awk '{print $1}')
          cache_key=$(echo "$line" | awk '{print $2}')
          
          # Check if cache is older than 7 days
          if [ -n "$cache_id" ] && [ "$cache_id" != "ID" ]; then
            echo "Deleting cache: $cache_key (ID: $cache_id)"
            gh cache delete "$cache_id" || echo "Failed to delete cache $cache_id"
          fi
        done
        
        # Clean up artifacts (older than 7 days)
        gh api repos/:owner/:repo/actions/artifacts --jq '.artifacts[] | select(.created_at | strptime("%Y-%m-%dT%H:%M:%SZ") | mktime < (now - 604800)) | .id' | while read artifact_id; do
          if [ -n "$artifact_id" ]; then
            echo "Deleting artifact: $artifact_id"
            gh api -X DELETE "repos/:owner/:repo/actions/artifacts/$artifact_id" || echo "Failed to delete artifact $artifact_id"
          fi
        done
        
        # Clean up workflow runs (keep only last 10)
        gh run list --limit 50 --json databaseId,status,conclusion | jq -r '.[] | select(.status == "completed" and .conclusion == "success") | .databaseId' | tail -n +11 | while read run_id; do
          if [ -n "$run_id" ]; then
            echo "Deleting workflow run: $run_id"
            gh run delete "$run_id" || echo "Failed to delete run $run_id"
          fi
        done
        
        echo "‚úÖ GitHub Actions cleanup completed"

    - name: Clean up old ECR images
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        echo "üßπ Cleaning up old ECR images..."
        
        # List of repositories to cleanup
        REPOSITORIES=(
          "knowledgebot-docling-unified"
          "knowledgebot-agent-query"
          "knowledgebot-s3-unified"
          "knowledgebot-pinecone-unified"
          "knowledgebot-neo4j-unified"
          "knowledgebot-dynamodb-crud"
          "knowledgebot-chat-generator"
          "knowledgebot-embedding-generator"
        )
        
        for repo in "${REPOSITORIES[@]}"; do
          echo "Processing repository: $repo"
          
          # Check if repository exists
          if aws ecr describe-repositories --repository-names "$repo" --region "$AWS_REGION" >/dev/null 2>&1; then
            # Get all images and keep only last 5
            aws ecr describe-images \
              --repository-name "$repo" \
              --region "$AWS_REGION" \
              --query 'imageDetails[*].[imageDigest,imagePushedAt]' \
              --output text | sort -k2 -r | tail -n +6 | while read digest pushed_at; do
                if [ -n "$digest" ]; then
                  echo "Deleting old image: $digest (pushed: $pushed_at)"
                  aws ecr batch-delete-image \
                    --repository-name "$repo" \
                    --region "$AWS_REGION" \
                    --image-ids imageDigest="$digest" || echo "Failed to delete image $digest"
                fi
              done
            
            # Apply lifecycle policy
            if [ -f "ecr-lifecycle-policy.json" ]; then
              aws ecr put-lifecycle-policy \
                --repository-name "$repo" \
                --region "$AWS_REGION" \
                --lifecycle-policy-text file://ecr-lifecycle-policy.json || echo "Failed to apply lifecycle policy to $repo"
            fi
          fi
        done
        
        echo "‚úÖ ECR cleanup completed"

  # Build all microservices in parallel
  build-services:
    needs: cleanup
    runs-on: ubuntu-latest
    environment: chatbot
    strategy:
      max-parallel: 3
      matrix:
        include:
          # Core unified services
          - name: docling-unified
            dockerfile: Dockerfile.docling-unified
            requirements: requirements-docling-unified.txt
            size: "~2.5GB"
            memory: 3008
            timeout: 900
          - name: agent-query
            dockerfile: Dockerfile.intelligent-agent
            requirements: requirements-intelligent-agent.txt
            size: "~200MB"
            memory: 1024
            timeout: 300
          
          # Supporting microservices
          - name: s3-unified
            dockerfile: Dockerfile.s3-unified
            requirements: requirements-s3-unified.txt
            size: "~85MB"
            memory: 256
            timeout: 30
          - name: pinecone-unified
            dockerfile: Dockerfile.pinecone-unified
            requirements: requirements-pinecone-unified.txt
            size: "~155MB"
            memory: 512
            timeout: 60
          - name: neo4j-unified
            dockerfile: Dockerfile.neo4j-unified
            requirements: requirements-neo4j-unified.txt
            size: "~155MB"
            memory: 512
            timeout: 60
          - name: dynamodb-crud
            dockerfile: Dockerfile.dynamodb-crud-layered
            requirements: requirements-dynamodb-crud.txt
            size: "~85MB"
            memory: 256
            timeout: 30
          - name: chat-generator
            dockerfile: Dockerfile.chat-generator-layered
            requirements: requirements-chat-generator.txt
            size: "~405MB"
            memory: 1024
            timeout: 300
          - name: embedding-generator
            dockerfile: Dockerfile.embedding-generator-layered
            requirements: requirements-embedding-generator.txt
            size: "~405MB"
            memory: 1024
            timeout: 120
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Verify AWS credentials
      run: |
        echo "üîç Verifying AWS credentials..."
        aws sts get-caller-identity
        echo "‚úÖ AWS credentials verified successfully"

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Clean up Docker and system memory
      if: matrix.name == 'docling-unified'
      run: |
        echo "üßπ Cleaning up Docker and system memory for large build..."
        
        # Stop all running containers
        docker stop $(docker ps -q) 2>/dev/null || true
        
        # Remove all containers
        docker rm $(docker ps -aq) 2>/dev/null || true
        
        # Remove unused images (keep only latest)
        docker image prune -f
        
        # Remove unused volumes
        docker volume prune -f
        
        # Remove unused networks
        docker network prune -f
        
        # Clear Docker build cache
        docker builder prune -f
        
        # Clear system cache
        sudo sync
        echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null
        
        # Show available memory
        free -h
        df -h
        
        echo "‚úÖ Memory cleanup completed"

    - name: Ensure ECR Repository Exists
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        echo "üîç Ensuring ECR repository exists for service: ${{ matrix.name }}"
        
        # Check if repository exists, create if it doesn't
        if ! aws ecr describe-repositories --repository-names $ECR_REPOSITORY_PREFIX-${{ matrix.name }} --region $AWS_REGION >/dev/null 2>&1; then
          echo "üì¶ Creating ECR repository: $ECR_REPOSITORY_PREFIX-${{ matrix.name }}"
          aws ecr create-repository \
            --repository-name $ECR_REPOSITORY_PREFIX-${{ matrix.name }} \
            --region $AWS_REGION \
            --image-scanning-configuration scanOnPush=true
        else
          echo "‚úÖ ECR repository already exists: $ECR_REPOSITORY_PREFIX-${{ matrix.name }}"
        fi

    - name: Build Service ${{ matrix.name }}
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        echo "üèóÔ∏è Building service: ${{ matrix.name }} (${{ matrix.size }})"
        
        # Check if files exist
        if [ ! -f "${{ matrix.dockerfile }}" ]; then
          echo "‚ùå Dockerfile not found: ${{ matrix.dockerfile }}"
          exit 1
        fi
        
        if [ ! -f "${{ matrix.requirements }}" ]; then
          echo "‚ùå Requirements file not found: ${{ matrix.requirements }}"
          exit 1
        fi
        
        # Build Docker image
        if [ "${{ matrix.name }}" == "docling-unified" ]; then
          echo "üöÄ Building large docling-unified with memory optimization..."
          # Use BuildKit for better memory management
          export DOCKER_BUILDKIT=1
          # Build with memory limits and optimization
          docker build \
            --memory=6g \
            --memory-swap=8g \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            -f ${{ matrix.dockerfile }} \
            -t $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:$IMAGE_TAG .
        else
          docker build -f ${{ matrix.dockerfile }} -t $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:$IMAGE_TAG .
        fi
        
        # Tag as latest
        docker tag $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:latest
        
        # Push both tags
        docker push $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:$IMAGE_TAG
        docker push $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:latest
        
        echo "‚úÖ Built and pushed service: ${{ matrix.name }}"
        
        # Clean up after large builds
        if [ "${{ matrix.name }}" == "docling-unified" ]; then
          echo "üßπ Cleaning up after large build..."
          # Remove the built image to free space
          docker rmi $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:$IMAGE_TAG || true
          docker rmi $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:latest || true
          # Clear build cache
          docker builder prune -f
          echo "‚úÖ Cleanup completed"
        fi

    - name: Deploy Service ${{ matrix.name }}
      if: github.ref == 'refs/heads/main'
      env:
        IMAGE_URI: ${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY_PREFIX }}-${{ matrix.name }}:${{ github.sha }}
      run: |
        echo "üöÄ Deploying ${{ matrix.name }} microservice"
        
        # Check if function exists
        if aws lambda get-function --function-name knowledgebot-${{ matrix.name }} >/dev/null 2>&1; then
          echo "üîÑ Updating existing microservice: knowledgebot-${{ matrix.name }}"
          
          # Update function code with retry logic
          echo "üîÑ Updating function code..."
          for attempt in 1 2 3 4 5; do
            echo "Attempt $attempt of 5"
            if aws lambda update-function-code \
              --function-name knowledgebot-${{ matrix.name }} \
              --image-uri $IMAGE_URI \
              --region $AWS_REGION; then
              echo "‚úÖ Function code updated successfully"
              break
            fi
            
            if [ $attempt -lt 5 ]; then
              wait_time=$((attempt * 30))
              echo "‚è≥ Waiting $wait_time seconds before retry..."
              sleep $wait_time
            else
              echo "‚ùå Failed to update function code after 5 attempts"
              exit 1
            fi
          done
          
          # Set environment variables based on service type
          case "${{ matrix.name }}" in
            "presigned-url")
              ENV_VARS="DOCUMENTS_BUCKET=${{ secrets.DOCUMENTS_BUCKET }}"
              ;;
            "docling-unified")
              ENV_VARS="DOCUMENTS_BUCKET=${{ secrets.DOCUMENTS_BUCKET }},PROCESSED_DOCUMENTS_BUCKET=${{ secrets.PROCESSED_DOCUMENTS_BUCKET }},CHUNKS_TABLE=${{ secrets.CHUNKS_TABLE }},PINECONE_UPSERT_FUNCTION=${{ secrets.PINECONE_UPSERT_FUNCTION }},NEO4J_WRITE_FUNCTION=${{ secrets.NEO4J_WRITE_FUNCTION }}"
              ;;
            "agent-query")
              ENV_VARS="PINECONE_SEARCH_FUNCTION=${{ secrets.PINECONE_SEARCH_FUNCTION }},NEO4J_SEARCH_FUNCTION=${{ secrets.NEO4J_SEARCH_FUNCTION }},DYNAMODB_READ_FUNCTION=${{ secrets.DYNAMODB_READ_FUNCTION }},S3_READER_FUNCTION=${{ secrets.S3_READER_FUNCTION }}"
              ;;
            "pinecone-search"|"pinecone-upsert")
              ENV_VARS="PINECONE_API_KEY=${{ secrets.PINECONE_API_KEY }},PINECONE_INDEX_NAME=${{ secrets.PINECONE_INDEX_NAME }},PINECONE_HOST=${{ secrets.PINECONE_HOST }},PINECONE_DIMENSIONS=${{ secrets.PINECONE_DIMENSIONS }},PINECONE_ENVIRONMENT=${{ secrets.PINECONE_ENVIRONMENT }},PINECONE_METRIC=${{ secrets.PINECONE_METRIC }}"
              ;;
            "neo4j-search"|"neo4j-write")
              ENV_VARS="NEO4J_URI=${{ secrets.NEO4J_URI }},NEO4J_USER=${{ secrets.NEO4J_USER }},NEO4J_PASSWORD=${{ secrets.NEO4J_PASSWORD }}"
              ;;
            "embedding-generator")
              ENV_VARS="OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
              ;;
            "chat-generator")
              ENV_VARS="OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
              ;;
            *)
              ENV_VARS=""
              ;;
          esac
          
          # Update function configuration with retry logic
          echo "üîÑ Updating function configuration..."
          for attempt in 1 2 3 4 5; do
            echo "Attempt $attempt of 5"
            if [ -n "$ENV_VARS" ]; then
              if aws lambda update-function-configuration \
                --function-name knowledgebot-${{ matrix.name }} \
                --memory-size ${{ matrix.memory }} \
                --timeout ${{ matrix.timeout }} \
                --environment Variables="{$ENV_VARS}" \
                --region $AWS_REGION; then
                echo "‚úÖ Function configuration updated successfully"
                break
              fi
            else
              if aws lambda update-function-configuration \
                --function-name knowledgebot-${{ matrix.name }} \
                --memory-size ${{ matrix.memory }} \
                --timeout ${{ matrix.timeout }} \
                --region $AWS_REGION; then
                echo "‚úÖ Function configuration updated successfully"
                break
              fi
            fi
            
            if [ $attempt -lt 5 ]; then
              wait_time=$((attempt * 30))
              echo "‚è≥ Waiting $wait_time seconds before retry..."
              sleep $wait_time
            else
              echo "‚ùå Failed to update function configuration after 5 attempts"
              exit 1
            fi
          done
        else
          echo "üÜï Creating new microservice: knowledgebot-${{ matrix.name }}"
          
          # Set environment variables based on service type
          case "${{ matrix.name }}" in
            "presigned-url")
              ENV_VARS="DOCUMENTS_BUCKET=${{ secrets.DOCUMENTS_BUCKET }}"
              ;;
            "docling-unified")
              ENV_VARS="DOCUMENTS_BUCKET=${{ secrets.DOCUMENTS_BUCKET }},PROCESSED_DOCUMENTS_BUCKET=${{ secrets.PROCESSED_DOCUMENTS_BUCKET }},CHUNKS_TABLE=${{ secrets.CHUNKS_TABLE }},PINECONE_UPSERT_FUNCTION=${{ secrets.PINECONE_UPSERT_FUNCTION }},NEO4J_WRITE_FUNCTION=${{ secrets.NEO4J_WRITE_FUNCTION }}"
              ;;
            "agent-query")
              ENV_VARS="PINECONE_SEARCH_FUNCTION=${{ secrets.PINECONE_SEARCH_FUNCTION }},NEO4J_SEARCH_FUNCTION=${{ secrets.NEO4J_SEARCH_FUNCTION }},DYNAMODB_READ_FUNCTION=${{ secrets.DYNAMODB_READ_FUNCTION }},S3_READER_FUNCTION=${{ secrets.S3_READER_FUNCTION }}"
              ;;
            "pinecone-search"|"pinecone-upsert")
              ENV_VARS="PINECONE_API_KEY=${{ secrets.PINECONE_API_KEY }},PINECONE_INDEX_NAME=${{ secrets.PINECONE_INDEX_NAME }},PINECONE_HOST=${{ secrets.PINECONE_HOST }},PINECONE_DIMENSIONS=${{ secrets.PINECONE_DIMENSIONS }},PINECONE_ENVIRONMENT=${{ secrets.PINECONE_ENVIRONMENT }},PINECONE_METRIC=${{ secrets.PINECONE_METRIC }}"
              ;;
            "neo4j-search"|"neo4j-write")
              ENV_VARS="NEO4J_URI=${{ secrets.NEO4J_URI }},NEO4J_USER=${{ secrets.NEO4J_USER }},NEO4J_PASSWORD=${{ secrets.NEO4J_PASSWORD }}"
              ;;
            "embedding-generator")
              ENV_VARS="OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
              ;;
            "chat-generator")
              ENV_VARS="OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}"
              ;;
            *)
              ENV_VARS=""
              ;;
          esac
          
          # Create new function
          if [ -n "$ENV_VARS" ]; then
            aws lambda create-function \
              --function-name knowledgebot-${{ matrix.name }} \
              --package-type Image \
              --code ImageUri=$IMAGE_URI \
              --role arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/lambda-execution-role \
              --memory-size ${{ matrix.memory }} \
              --timeout ${{ matrix.timeout }} \
              --environment Variables="{$ENV_VARS}" \
              --region $AWS_REGION
          else
            aws lambda create-function \
              --function-name knowledgebot-${{ matrix.name }} \
              --package-type Image \
              --code ImageUri=$IMAGE_URI \
              --role arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/lambda-execution-role \
              --memory-size ${{ matrix.memory }} \
              --timeout ${{ matrix.timeout }} \
              --region $AWS_REGION
          fi
        fi
        
        echo "‚úÖ Deployed knowledgebot-${{ matrix.name }} successfully"

    - name: Output build details
      run: |
        echo "üì¶ Service: $ECR_REGISTRY/$ECR_REPOSITORY_PREFIX-${{ matrix.name }}:$IMAGE_TAG"
        echo "üìè Size: ${{ matrix.size }}"
        echo "üíæ Memory: ${{ matrix.memory }}MB"
        echo "‚è±Ô∏è Timeout: ${{ matrix.timeout }}s"

  # Summary job
  deployment-summary:
    needs: [cleanup, build-services]
    runs-on: ubuntu-latest
    environment: chatbot
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Unified Architecture Deployment Summary
      run: |
        echo "üéâ KnowledgeBot Unified Architecture Deployment Complete!"
        echo ""
        echo "üèóÔ∏è Core Unified Services:"
        echo "  ‚Ä¢ knowledgebot-docling-unified (3008MB, 900s, ~2.5GB) - Complete document processing pipeline"
        echo "  ‚Ä¢ knowledgebot-agent-query (1024MB, 300s, ~200MB) - Intelligent query handling"
        echo ""
        echo "üìã Supporting Micro-Services (6 services):"
        echo ""
        echo "üîß Core Services:"
        echo "  ‚Ä¢ knowledgebot-s3-unified (256MB, 30s, ~85MB) - Presigned URLs, reading, listing"
        echo "  ‚Ä¢ knowledgebot-pinecone-unified (512MB, 60s, ~155MB) - Search and upsert operations"
        echo "  ‚Ä¢ knowledgebot-neo4j-unified (512MB, 60s, ~155MB) - Graph search and write operations"
        echo "  ‚Ä¢ knowledgebot-dynamodb-crud (256MB, 30s, ~85MB)"
        echo "  ‚Ä¢ knowledgebot-embedding-generator (1024MB, 120s, ~405MB)"
        echo "  ‚Ä¢ knowledgebot-chat-generator (1024MB, 300s, ~405MB)"
        echo ""
        echo "‚ö° Unified Architecture Benefits:"
        echo "  ‚Ä¢ S3 direct trigger for document processing"
        echo "  ‚Ä¢ Single unified docling image"
        echo "  ‚Ä¢ Intelligent agent query handling"
        echo "  ‚Ä¢ Complete processing pipeline"
        echo "  ‚Ä¢ GitHub Actions cleanup integration (cache, artifacts, runs)"
        echo "  ‚Ä¢ ECR image cleanup (keeps last 5 images)"
        echo "  ‚Ä¢ Streamlined deployment process"
        echo ""
        echo "üìä Total Architecture:"
        echo "  ‚Ä¢ Total Services: 8 (~2.8GB)"
        echo "  ‚Ä¢ Build Time: ~6 minutes (parallel)"
        echo "  ‚Ä¢ Redundancy: Minimal (unified approach)"
        echo ""
        echo "üîó ECR Repositories:"
        echo "  ‚Ä¢ Registry: ${{ env.ECR_REGISTRY }}"
        echo "  ‚Ä¢ Prefix: ${{ env.ECR_REPOSITORY_PREFIX }}"
        echo "  ‚Ä¢ Total Images: 8 services"
